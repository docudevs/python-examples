{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184a31a7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4bafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the SDK if needed\n",
    "# %pip install docu-devs-api-client pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from docudevs import DocuDevsClient\n",
    "\n",
    "API_KEY = os.getenv(\"DOCUDEVS_API_KEY\", \"your-api-key-here\")\n",
    "client = DocuDevsClient(token=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be740efd",
   "metadata": {},
   "source": [
    "## The Problem: Long Documents\n",
    "\n",
    "Long documents (50+ pages) are tough for AI to process in one shot:\n",
    "- Context windows get overloaded\n",
    "- The AI loses track of information\n",
    "- You get incomplete or garbled results\n",
    "\n",
    "**Map-Reduce** solves this by:\n",
    "1. Breaking the document into chunks (e.g., 10 pages each)\n",
    "2. Processing each chunk independently  \n",
    "3. Combining the results into a clean output\n",
    "\n",
    "**Real Example**: We'll extract SLA service credit tables from the Azure SLA document - \n",
    "a multi-page document where each service has its own credit table scattered throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755fcc2",
   "metadata": {},
   "source": [
    "## Define Your Schema\n",
    "\n",
    "For this example, we want to extract service credit tables that look like:\n",
    "\n",
    "| Uptime Percentage | Service Credit |\n",
    "|-------------------|----------------|\n",
    "| < 99.9%           | 25%            |\n",
    "| < 99%             | 50%            |\n",
    "| < 95%             | 100%           |\n",
    "\n",
    "Each service in the Azure SLA has its own table like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe180bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class ServiceCreditEntry(BaseModel):\n",
    "    \"\"\"A single row in the service credit table.\"\"\"\n",
    "    uptime_percentage: str = Field(\n",
    "        alias=\"UptimePercentage\",\n",
    "        description=\"Uptime threshold, e.g. '< 99.9%' or '>= 99.95%'\"\n",
    "    )\n",
    "    service_credit: str = Field(\n",
    "        alias=\"ServiceCredit\",\n",
    "        description=\"Credit percentage, e.g. '25%'\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ServiceCredits(BaseModel):\n",
    "    \"\"\"The service credits table for a service.\"\"\"\n",
    "    entries: list[ServiceCreditEntry] = Field(\n",
    "        default_factory=list,\n",
    "        alias=\"Service Credits\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ServiceSla(BaseModel):\n",
    "    \"\"\"SLA information for a single Azure service.\"\"\"\n",
    "    service_name: str = Field(\n",
    "        alias=\"Service Name\",\n",
    "        description=\"Name of the Azure service\"\n",
    "    )\n",
    "    service_credits: ServiceCredits = Field(default_factory=ServiceCredits)\n",
    "\n",
    "\n",
    "# Generate the schema for an array of ServiceSla objects\n",
    "schema_json = json.dumps({\n",
    "    \"type\": \"array\",\n",
    "    \"items\": ServiceSla.model_json_schema()\n",
    "})\n",
    "\n",
    "print(\"Schema preview:\")\n",
    "print(json.dumps(ServiceSla.model_json_schema(), indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4783e7d",
   "metadata": {},
   "source": [
    "## Process with Map-Reduce\n",
    "\n",
    "Key parameters:\n",
    "- `pages_per_chunk`: How many pages to process at once\n",
    "- `overlap_pages`: Pages to repeat between chunks (catches data spanning pages)\n",
    "- `dedup_key`: Which field to use for deduplication (required when overlap > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Azure SLA document\n",
    "from pathlib import Path\n",
    "\n",
    "sla_doc_path = Path(\"docs/azure-sla.docx\")\n",
    "sla_doc = sla_doc_path.read_bytes()\n",
    "print(f\"Loaded Azure SLA: {len(sla_doc):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74283bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the map-reduce job\n",
    "job_id = await client.submit_and_process_document_map_reduce(\n",
    "    document=sla_doc,\n",
    "    document_mime_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "    \n",
    "    # Row extraction (runs on each chunk)\n",
    "    prompt=\"Extract all service SLAs with their service credit tables. Each service has a name and a table showing uptime percentages and corresponding service credits.\",\n",
    "    schema=schema_json,\n",
    "    \n",
    "    # Chunking configuration  \n",
    "    pages_per_chunk=10,    # Process 10 pages at a time\n",
    "    overlap_pages=1,       # Overlap to catch services spanning pages\n",
    "    dedup_key=\"Service Name\",  # Use service name to deduplicate\n",
    "    \n",
    "    # Use the smaller model for faster processing\n",
    "    llm=\"MINI\",\n",
    ")\n",
    "\n",
    "print(f\"Job submitted: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c41311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for results\n",
    "result = await client.wait_until_ready(job_id, result_format=\"json\", poll_interval=10, timeout=600)\n",
    "\n",
    "# Parse all extracted SLAs\n",
    "records = result.get(\"records\", [])\n",
    "print(f\"Extracted {len(records)} chunks of services\")\n",
    "\n",
    "# Flatten all services from all chunks\n",
    "all_services: list[ServiceSla] = []\n",
    "for record in records:\n",
    "    if isinstance(record, list):\n",
    "        for item in record:\n",
    "            all_services.append(ServiceSla.model_validate(item))\n",
    "    elif isinstance(record, dict) and \"Service Name\" in record:\n",
    "        all_services.append(ServiceSla.model_validate(record))\n",
    "\n",
    "print(f\"Total services found: {len(all_services)}\\n\")\n",
    "\n",
    "# Display a few examples\n",
    "for sla in all_services[:3]:\n",
    "    print(f\"ðŸ“‹ {sla.service_name}\")\n",
    "    for entry in sla.service_credits.entries[:3]:\n",
    "        print(f\"   {entry.uptime_percentage}: {entry.service_credit} credit\")\n",
    "    if len(sla.service_credits.entries) > 3:\n",
    "        print(f\"   ... and {len(sla.service_credits.entries) - 3} more tiers\")\n",
    "    print()\n",
    "\n",
    "if len(all_services) > 3:\n",
    "    print(f\"... and {len(all_services) - 3} more services\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c3347",
   "metadata": {},
   "source": [
    "## Monitoring Progress\n",
    "\n",
    "Map-reduce jobs process chunks in parallel, so they can take a while. Here's how to monitor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_with_progress(document_bytes: bytes, mime_type: str, schema: str, prompt: str):\n",
    "    \"\"\"Process a document and show progress updates.\"\"\"\n",
    "    \n",
    "    job_id = await client.submit_and_process_document_map_reduce(\n",
    "        document=document_bytes,\n",
    "        document_mime_type=mime_type,\n",
    "        prompt=prompt,\n",
    "        schema=schema,\n",
    "        pages_per_chunk=10,\n",
    "        overlap_pages=1,\n",
    "        dedup_key=\"Service Name\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Job started: {job_id}\")\n",
    "    \n",
    "    while True:\n",
    "        status_response = await client.status(job_id)\n",
    "        job_status = status_response.parsed\n",
    "        \n",
    "        if job_status.status == \"COMPLETED\":\n",
    "            print(\"\\nâœ“ Done!\")\n",
    "            break\n",
    "        elif job_status.status == \"ERROR\":\n",
    "            print(f\"\\nâœ— Error: {job_status.error}\")\n",
    "            break\n",
    "        \n",
    "        # Check map-reduce progress if available\n",
    "        mr_status = getattr(job_status, \"map_reduce_status\", None)\n",
    "        if mr_status:\n",
    "            completed = getattr(mr_status, \"completed_chunks\", 0)\n",
    "            total = getattr(mr_status, \"total_chunks\", 0)\n",
    "            print(f\"\\rProgress: {completed}/{total} chunks\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(f\"\\rStatus: {job_status.status}\", end=\"\", flush=True)\n",
    "        \n",
    "        await asyncio.sleep(2)\n",
    "    \n",
    "    return await client.wait_until_ready(job_id, result_format=\"json\")\n",
    "\n",
    "# Example usage (commented out to avoid re-running):\n",
    "# result = await process_with_progress(\n",
    "#     sla_doc,\n",
    "#     \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "#     schema_json,\n",
    "#     \"Extract all service SLAs with their service credit tables.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45d06e",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "| Option | What it does | When to use |\n",
    "|--------|--------------|-------------|\n",
    "| `pages_per_chunk` | Pages processed together | Larger = more context but slower. 5-10 is a good start. |\n",
    "| `overlap_pages` | Pages repeated between chunks | Use 1 when data might span page breaks |\n",
    "| `dedup_key` | Field for deduplication | Required when overlap > 0. Pick a unique field. |\n",
    "| `llm` | Model to use | `\"MINI\"` for faster/cheaper, default for best quality |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674c863",
   "metadata": {},
   "source": [
    "## Tips for Map-Reduce\n",
    "\n",
    "1. **Choose the right chunk size**: Start with 5-10 pages. If you're missing context, increase it.\n",
    "\n",
    "2. **Always use dedup_key with overlap**: Otherwise you'll get duplicate entries from overlapping pages.\n",
    "\n",
    "3. **Pick a truly unique dedup field**: Service name, ID, or SKU work well. Generic descriptions don't.\n",
    "\n",
    "4. **Use MINI for large documents**: Faster and cheaper for straightforward extraction tasks.\n",
    "\n",
    "5. **Test on a sample first**: Try a few pages to validate your schema before processing the whole doc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b516051",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[Knowledge Search](03-knowledge-search.ipynb)**: Enrich extractions with your own reference data\n",
    "- **[Operations](04-operations.ipynb)**: Run error analysis and ask follow-up questions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
